<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Design Credits: Jon Barron, Deepak Pathak, Saurabh Gupta and Aditya Kusupati*/
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 400
    }

    heading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 19px;
      font-weight: 1000
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 800
    }

    strongred {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      color: 'red';
      font-size: 16px
    }

    sectionheading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 22px;
      font-weight: 600
    }

    pageheading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 38px;
      font-weight: 400
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Eungyeup Kim</title>
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
  <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center">
            <pageheading>Eungyeup Kim</pageheading><br>
            <b>email</b>:
            eungyeuk@cs.cmu.edu
            <font id="email" style="display:inline;">
              <noscript><i>Please enable Javascript to view</i></noscript>
            </font>
            <!--      <script>
     emailScramble = new scrambledString(document.getElementById('email'),
          'emailScramble', 'aecsukthoad@wssg.iutn.uinp',
          [14,24,10,15,26,1,7,16,21,6,25,9,13,3,11,19,23,17,4,20,22,12,2,8,18,5]);
     </script> -->
          </p>

          <tr>
            <td width="32%" valign="top"><a href="#Bio"><img src="images/eungyeup_profile.jpeg" width="100%"
                  style="border-radius:15px"></a>
              <p align=center>
                <a href="https://drive.google.com/file/d/1_y5WPX2cCsHSfmhv5BqxHMCaMZiu6M4z/view" target="_blank">CV</a> | <a
                  href="https://scholar.google.com/citations?user=RtxkcwYAAAAJ&hl=en" target="_blank">Scholar</a> | <a
                  href="https://github.com/EungyeupKim" target="_blank">Github</a> <br>
              </p>
            </td>
            <td width="68%" valign="top" align="justify" id="Bio">
                <p> I am a second-year PhD student in <a href="https://www.cs.cmu.edu/">Computer Science Department</a> at Carnegie Mellon University, advised by Prof. <a href="https://zicokolter.com/">Zico Kolter</a>.
                    My research interest lies in robust ML under distribution shifts and foundation models.
                    I did my Master's studies at Korea Advanced Institute of Science and Technology (<a href="https://www.kaist.ac.kr/en/">KAIST</a>) under Prof. <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>. </p>

            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="Preprints">
              <sectionheading>&nbsp;&nbsp;Preprints</sectionheading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="33%" valign="top" align="center"><a href="" target="_blank"><img
                  src="images/tta_agl.png" alt="TTAAGL" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="" target="_blank" id="ttaagl">
                  <heading>Reliable Test-Time Adaptation via Agreement-on-the-Line
                  </heading>
                </a><br>
                <strong>Eungyeup Kim</strong>, <a href="https://eric-mingjie.github.io/" target="_blank"> Mingjie Sun
                  </a>, <a href="https://www.cs.cmu.edu/~aditirag/" target="_blank"> Aditi Raghunathan</a>, <a
                  href="https://zicokolter.com/" target="_blank"> Zico Kolter</a><br>
              </p>

              <div class="paper" id="ttaagl">
                <a href="javascript:toggleblock('ttaaglabs')">abstract</a> /
<!--                <a href="" target="_blank">paper</a>-->
                paper
                <br>

                <p align="justify"> <i id="ttaaglabs">Test-time adaptation (TTA) methods aim to improve robustness to distribution shifts by adapting models using unlabeled data from the shifted test distribution.
                        However, there remain unresolved challenges that undermine the reliability of TTA, which include difficulties in evaluating TTA performance, miscalibration after TTA, and  unreliable hyperparameter tuning for adaptation.
                        In this work, we make a notable and surprising observation that TTAed models strongly show the agreement-on-the-line phenomenon (Baek et al., 2022) across a wide range of distribution shifts.
                        We find such linear trends occur consistently in a wide range of models adapted with various hyperparameters, and persist in distributions where the phenomenon fails to hold in vanilla models (i.e., before adaptation).
                        We leverage these observations to make TTA methods more reliable in three perspectives: (i) estimating OOD accuracy (without labeled data) to determine when TTA helps and when it hurts, (ii)
                        calibrating TTAed models without label information, and (iii) reliably determining hyperparameters for TTA without any labeled validation data.
                        Through extensive experiments, we demonstrate that various TTA methods can be precisely evaluated, both in terms of their improvements and degradations.
                        Moreover, our proposed methods on unsupervised calibration and hyperparameters tuning for TTA achieve results close to the ones assuming access to ground-truth labels, in terms of both OOD accuracy and calibration error.</i>
                </p>

          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td id="ConfPublications">
              <sectionheading>&nbsp;&nbsp;Publications</sectionheading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://papers.nips.cc/paper/2021/file/d360a502598a4b64b936683b44a5523a-Paper.pdf" target="_blank"><img
                  src="images/disentangle-augmentation-debiasing.png" alt="DISENT" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://papers.nips.cc/paper/2021/file/d360a502598a4b64b936683b44a5523a-Paper.pdf" target="_blank" id="disent">
                  <heading>Learning Debiased Representation via Disentangled Feature Augmentation</heading>
                </a><br>
                Jungsoo Lee<sup>*</sup>, <strong>Eungyeup Kim<sup>*</sup></strong> Juyoung Lee, Jihyeon Lee, <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Jaegul Choo</a><br>
                Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021 (<span
                  style="color:darkred;"><strong>Oral</strong></span>).<br>
              </p>

              <div class="paper" id="disent">
                <a href="javascript:toggleblock('disentabs')">abstract</a> /
                <a href="https://papers.nips.cc/paper/2021/file/d360a502598a4b64b936683b44a5523a-Paper.pdf" target="_blank">paper</a> /
                <a href="https://github.com/kakaoenterprise/Learning-Debiased-Disentangled" target="_blank">code</a> /
                <br>

                <p align="justify"> <i id="disentabs">Image classification models tend to make decisions based on peripheral
                    attributes of data items that have strong correlation with a target variable (i.e., dataset bias).
                    These biased models suffer from the poor generalization capability when evaluated on unbiased datasets.
                    Existing approaches for debiasing often identify and emphasize those samples with no such correlation
                    (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting
                    samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches.
                    This paper first presents an empirical analysis revealing that training with "diverse" bias-conflicting
                    samples beyond a given training set is crucial for debiasing as well as the generalization capability.
                    Based on this observation, we propose a novel feature-level data augmentation technique in order to
                    synthesize diverse bias-conflicting samples. To this end, our method learns the disentangled
                    representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and
                    (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned
                    samples, the bias attributes of which have strong correlation with the target variable. Using the disentangled
                    representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of
                    bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting
                    features during the training, our approach achieves superior classification accuracy and debiasing
                    results against the existing baselines on both synthetic as well as a real-world dataset.</i></p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Deep_Edge-Aware_Interactive_Colorization_Against_Color-Bleeding_Effects_ICCV_2021_paper.pdf"
                target="_blank"><img src="images/edge-enhancing-colorization_teaser.gif" alt="DEEPEDGE" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Deep_Edge-Aware_Interactive_Colorization_Against_Color-Bleeding_Effects_ICCV_2021_paper.pdf" target="_blank" id="deepedge">
                  <heading>Deep Edge-Aware Interactive Colorization against Color Bleeding Effects</heading>
                </a><br>
                <strong>Eungyeup Kim<sup>*</sup></strong>, Sanghyeon Lee<sup>*</sup>, Jeonghoon Park<sup>*</sup>, Somi Choi,
                  Choonghyun Seo, <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Jaegul Choo</a> <br>
                IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021(<span
                  style="color:darkred;"><strong>Oral</strong></span>).<br>
              </p>

              <div class="paper" id="deepedge">
                <a href="javascript:toggleblock('deepedgeabs')">abstract</a> /
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Deep_Edge-Aware_Interactive_Colorization_Against_Color-Bleeding_Effects_ICCV_2021_paper.pdf" target="_blank">paper</a> /
                <a href="https://eungyeupkim.github.io/edge-enhancing-colorization/" target="_blank">project</a>
                <br>

                <p align="justify"> <i id="deepedgeabs">Deep image colorization networks often suffer from the color-bleeding
                    artifact, a problematic color spreading near the boundaries between adjacent objects. The color-bleeding
                    artifacts debase the reality of generated outputs, limiting the applicability of colorization models
                    on a practical application. Although previous approaches have tackled this problem in an automatic
                    manner, they often generate imperfect outputs because their enhancements are available only in limited
                    cases, such as having a high contrast of gray-scale value in an input image. Instead, leveraging user
                    interactions would be a promising approach, since it can help the edge correction in the desired
                    regions. In this paper, we propose a novel edge-enhancing framework for the regions of interest, by
                    utilizing user scribbles that indicate where to enhance. Our method requires minimal user effort to
                    obtain satisfactory enhancements. Experimental results on various datasets demonstrate that our
                    interactive approach has outstanding performance in improving color-bleeding artifacts against the
                    existing baselines.</i></p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_BiaSwap_Removing_Dataset_Bias_With_Bias-Tailored_Swapping_Augmentation_ICCV_2021_paper.pdf" target="_blank"><img src="images/biaswap.png"
                  alt="BIASWAP" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_BiaSwap_Removing_Dataset_Bias_With_Bias-Tailored_Swapping_Augmentation_ICCV_2021_paper.pdf" target="_blank" id="biaswap">
                  <heading>BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation</heading>
                </a><br>
                <strong>Eungyeyp Kim</strong><sup>*</sup>, Jihyeon Lee<sup>*</sup>, <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Jaegul Choo</a> <br>
                IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021<br>
              </p>

              <div class="paper" id="biaswap">
                <a href="javascript:toggleblock('biaswapabs')">abstract</a> /
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_BiaSwap_Removing_Dataset_Bias_With_Bias-Tailored_Swapping_Augmentation_ICCV_2021_paper.pdf" target="_blank">paper</a>
                <br>

                <p align="justify"> <i id="biaswapabs">Deep neural networks often make decisions based on
                    the spurious correlations inherent in the dataset, failing
                    to generalize in an unbiased data distribution. Although
                    previous approaches pre-define the type of dataset bias to
                    prevent the network from learning it, recognizing the bias
                    type in the real dataset is often prohibitive. This paper proposes a novel bias-tailored augmentation-based approach,
                    BiaSwap, for learning debiased representation without requiring supervision on the bias type. Assuming that the
                    bias corresponds to the easy-to-learn attributes, we sort the
                    training images based on how much a biased classifier can
                    exploits them as shortcut and divide them into bias-guiding
                    and bias-contrary samples in an unsupervised manner. Afterwards, we integrate the style-transferring module of the
                    image translation model with the class activation maps of
                    such biased classifier, which enables to primarily transfer the bias attributes learned by the classifier. Therefore,
                    given the pair of bias-guiding and bias-contrary, BiaSwap
                    generates the bias-swapped image which contains the bias
                    attributes from the bias-contrary images, while preserving
                    bias-irrelevant ones in the bias-guiding images. Given such
                    augmented images, BiaSwap demonstrates the superiority
                    in debiasing against the existing baselines over both synthetic and real-world datasets.
                    Even without careful supervision on the bias, BiaSwap achieves a remarkable performance on both unbiased and bias-guiding samples, implying the improved generalization capability of the model.</i></p>

          </tr>

          <tr>
            <td width="33%" valign="top" align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Reference-Based_Sketch_Image_Colorization_Using_Augmented-Self_Reference_and_Dense_Semantic_CVPR_2020_paper.pdf" target="_blank"><img
                  src="images/reference-sketch-colorization_teaser.gif" alt="REF" width="100%" style="border-radius:15px"></a>
            <td width="67%" valign="top">
              <p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Reference-Based_Sketch_Image_Colorization_Using_Augmented-Self_Reference_and_Dense_Semantic_CVPR_2020_paper.pdf" target="_blank" id="ref">
                  <heading>Reference-Based Sketch Image Colorization Using Augmented-Self Reference and Dense Semantic Correspondence</heading>
                </a><br>
                Junsoo Lee<sup>*</sup>, <strong>Eungyeup Kim</strong><sup>*</sup>, Yunsung Lee, Dongjun Kim, Jaehyuk Chang, <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Jaegul Choo</a> <br>
                IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020<br>
              </p>

              <div class="paper" id="ref">
                <a href="javascript:toggleblock('refabs')">abstract</a> /
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Reference-Based_Sketch_Image_Colorization_Using_Augmented-Self_Reference_and_Dense_Semantic_CVPR_2020_paper.pdf" target="_blank">paper</a> /
                <a href="https://ssuhan.github.io/RSC_CVPR20/" target="_blank">project</a> /
                <a href="https://drive.google.com/file/d/1NjuESkJYZnKgfNg_k_5JFMYsxawlsnt1/view?usp=sharing" target="_blank">video</a>
                <br>

                <p align="justify"> <i id="refabs">This paper tackles the automatic colorization task of a sketch image
                    given an already-colored reference image. Colorizing a sketch image is in high demand in comics, animation,
                    and other content creation applications, but it suffers from information scarcity of a sketch image.
                    To address this, a reference image can render the colorization process in a reliable and user-driven
                    manner. However, it is difficult to prepare for a training data set that has a sufficient amount of
                    semantically meaningful pairs of images as well as the ground truth for a colored image reflecting a
                    given reference (e.g., coloring a sketch of an originally blue car given a reference green car). To
                    tackle this challenge, we propose to utilize the identical image with geometric distortion as a virtual
                    reference, which makes it possible to secure the ground truth for a colored output image. Furthermore,
                    it naturally provides the ground truth for dense semantic correspondence, which we utilize in our
                    internal attention mechanism for color transfer from reference to sketch input. We demonstrate the
                    effectiveness of our approach in various types of sketch image colorization via quantitative as well
                    as qualitative evaluation against existing methods..</i></p>

          </tr>
        </table>

      </td>
    </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('ttaaglabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('disentabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('deepedgeabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('biaswapabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('refabs');
  </script>
</body>

</html>
